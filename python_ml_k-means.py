# -*- coding: utf-8 -*-
"""
Created on Tue Nov 14 10:53:28 2023

@author: Marko
"""

# К-означає
# K-means — це метод неконтрольованого навчання для кластеризації точок даних. Алгоритм ітеративно ділить точки даних на K кластерів шляхом мінімізації дисперсії в кожному кластері.
# Тут ми покажемо вам, як оцінити найкраще значення K за допомогою методу ліктя, а потім використаємо кластеризацію K-середніх, щоб згрупувати точки даних у кластери.

# Як це працює?
# По-перше, кожна точка даних випадковим чином призначається одному з K кластерів. Потім ми обчислюємо центроїд (функціонально центр) кожного кластера та перепризначаємо кожну точку даних кластеру з найближчим центроїдом. Ми повторюємо цей процес, доки призначення кластерів для кожної точки даних не перестануть змінюватися.
# K-означає кластеризацію. Потрібно вибрати K, кількість кластерів, у які ми хочемо згрупувати дані. Метод ліктя дає нам змогу побудувати графік інерції (метрика на основі відстані) і візуалізувати точку, у якій вона починає лінійно зменшуватися. Ця точка називається «ударом» і є хорошою оцінкою найкращого значення K на основі наших даних.
import matplotlib.pyplot as plt
# Створюйте масиви, які нагадують дві змінні в наборі даних. Зауважте, що хоча ми використовуємо лише дві змінні, цей метод працюватиме з будь-якою кількістю змінних:
x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
# Перетворіть дані на набір точок:
plt.scatter(x, y)
plt.show()

# Тепер ми використовуємо метод ліктя, щоб візуалізувати проміжок для різних значень K:
from sklearn.cluster import KMeans

data = list(zip(x, y))
inertias = []
# Щоб знайти найкраще значення для K, нам потрібно провести K-середні для наших даних для діапазону можливих значень. У нас є лише 10 точок даних, тому максимальна кількість кластерів дорівнює 10. Отже, для кожного значення K у діапазоні (1,11) ми навчаємо модель K-середніх і будуємо інтерцію для такої кількості кластерів:
for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# Метод ліктя показує, що 2 є хорошим значенням для K, тому ми перенавчаємося та візуалізуємо результат:
# Ми бачимо, що «лікоть» на графіку вище (де інтерія стає більш лінійною) знаходиться на K=2. Потім ми можемо ще раз підібрати наш алгоритм K-середніх і побудувати графік різних кластерів, призначених даним:    
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

plt.scatter(x, y, c=kmeans.labels_)
plt.show()

