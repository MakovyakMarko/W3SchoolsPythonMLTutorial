# -*- coding: utf-8 -*-
"""
Created on Tue Oct 31 09:52:12 2023

@author: Marko
"""
#Створення матриці плутанини
#Матриці плутанини можна створити за допомогою прогнозів, зроблених на основі логістичної регресії.
#  Наразі ми будемо генерувати фактичні та прогнозовані значення за допомогою NumPy:

import numpy
# Далі нам потрібно буде згенерувати числа для «фактичних» і «прогнозованих» значень.
actual = numpy.random.binomial(1,0.9,size=1000)
predicted = numpy.random.binomial(1,0.9,size=1000)

# Щоб створити матрицю плутанини, нам потрібно імпортувати показники з модуля sklearn.
from sklearn import metrics

# Після імпортування показників ми можемо використовувати функцію матриці плутанини для наших фактичних і прогнозованих значень.
confusion_matrix = metrics.confusion_matrix(actual, predicted)

# Щоб створити візуальне відображення з більшою інтерпретацією, нам потрібно перетворити таблицю на відображення матриці плутанини.
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels = [False,True])

# Для візуалізації дисплея потрібно імпортувати pyplot з matplotlib.
import matplotlib.pyplot as plt

# Нарешті, щоб відобразити графік, ми можемо використати функції plot() і show() із pyplot.
cm_display.plot()
plt.show()

# Пояснення результатів
# Створена матриця плутанини складається з чотирьох різних квадрантів:

# Помилково-негативний (верхній лівий квадрант)
# Помилково-позитивний (верхній правий квадрант)
# Справжній негативний (нижній лівий квадрант)
# Справжній позитивний (нижній правий квадрант)
# True означає, що значення були точно передбачені, False означає, що була помилка або неправильний прогноз.

# Тепер, коли ми створили матрицю плутанини, ми можемо розрахувати різні заходи для кількісної оцінки якості моделі. Спочатку розглянемо точність.

# Точність
# Точність вимірює, наскільки часто модель є правильною.

# Як розрахувати
# (Істинний позитивний + Справжній негативний) / Загальні прогнози
accuracy = metrics.accuracy_score(actual, predicted)
print(accuracy)
# Який відсоток із прогнозованих позитивів є справді позитивним?

# Як розрахувати
# Справжній позитивний / (справжній позитивний + хибний позитивний)

# Точність не оцінює правильно передбачені негативні випадки:
precision = metrics.precision_score(actual,predicted)
print(precision)

# Чутливість (відклик)
# З усіх позитивних випадків, який відсоток прогнозовано позитивних?

# Чутливість (іноді її називають Recall) вимірює, наскільки добре модель прогнозує позитивні результати.

# Це означає, що він розглядає справжні позитивні та хибні негативні результати (це позитивні результати, які неправильно передбачили як негативні).

# Як розрахувати
# Справжній позитивний / (справжній позитивний + помилково негативний)

# Чутливість добре допомагає зрозуміти, наскільки добре модель прогнозує щось позитивне:
sensitivity_recall = metrics.recall_score(actual, predicted)
print(sensitivity_recall)

# Специфіка
# Наскільки добре модель прогнозує негативні результати?

# Специфічність подібна до чутливості, але розглядає її з точки зору негативних результатів.

# Як розрахувати
# Істинно негативний / (істинно негативний + хибно позитивний)

# Оскільки це прямо протилежність Recall, ми використовуємо функцію recall_score, беручи протилежну мітку позиції:
specificity = metrics.recall_score(actual, predicted, pos_label=0)
print(specificity)

# F-бал
# F-показник – це «середнє гармонійне» точності та чутливості.

# Він розглядає випадки як хибнопозитивних, так і хибнонегативних результатів і підходить для незбалансованих наборів даних.

# Як розрахувати
# 2 * ((Точність * Чутливість) / (Точність + Чутливість))

# Ця оцінка не враховує справжні негативні значення:
F1_score = metrics.f1_score(actual, predicted)
print(F1_score)

print({"Accuracy":accuracy,"Precision":precision,"Sensitivity_recall":sensitivity_recall,"Specificity":specificity,"F1_score":F1_score})